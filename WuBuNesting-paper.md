%%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#9966FF', 'secondaryColor': '#D6C0FF', 'tertiaryColor': '#FFF', 'primaryTextColor': '#333', 'secondaryTextColor': '#555', 'tertiaryTextColor': '#777', 'primaryBorderColor': '#9966FF', 'secondaryBorderColor': '#D6C0FF', 'tertiaryBorderColor': '#EEE', 'lineColor': '#9966FF', 'clusterBkg': '#F5F0FF', 'mainBkg': '#FFFFFF', 'nodeBorder': '#9966FF', 'edgeLabelBackground':'#F5F0FF'}}}%%

# WuBu Nesting: An Adaptive Multi-Scale Geometric Framework

**Abstract**

Representing complex data exhibiting deep, multi-scale hierarchical structures alongside inherent rotational or orientational properties poses a significant challenge for conventional deep learning models operating in Euclidean or spherical spaces. While hyperbolic geometry provides a powerful tool for embedding single-level hierarchies due to its negative curvature and exponential volume growth, and quaternions offer an efficient algebra for rotations in Euclidean tangent spaces, integrating these capabilities for recursive, multi-scale structures remains largely unexplored. We propose a novel conceptual framework, **WuBu Nesting**, designed to bridge this gap. WuBu Nesting envisions an adaptive, recursive geometric structure where hyperbolic spaces (specifically, Poincaré ball models) are nested within each other, analogous to Russian dolls ($\mathbb{H}^{n_1} \supset \mathbb{H}^{n_2} \supset \dots$). This hierarchical nesting creates a pseudo-high-dimensional space capable of capturing complex structural dependencies across multiple scales. Each level within this nested structure is characterized by learnable scale parameters ($s_i$) that determine its relative geometric size and influence the local metric. Furthermore, each level can host specialized, learnable boundary sub-manifolds (e.g., lower-dimensional hyperbolic disks visualized as "circles") representing distinct substructures or feature clusters pertinent to that scale. Crucially, transitions between these nested levels are mediated by diverse, learnable transformation functions. These functions, operating between the tangent spaces associated with the hyperbolic levels, can incorporate quaternion operations for explicitly modeling rotations, alongside simpler projections, probabilistic Gaussian mappings, or potentially even learned "bridge manifolds" connecting disparate regions across scales. This framework aims to offer unprecedented flexibility and a strong geometric inductive bias for modeling intricate data with heterogeneous geometric properties. We outline the conceptual architecture, detail the potential benefits for multi-resolution analysis and cross-scale modeling, discuss the significant mathematical and implementation challenges, and propose future research directions.

**1. Introduction**

The success of deep learning often relies on learning effective representations in high-dimensional vector spaces. However, standard Euclidean spaces, characterized by their flat geometry and polynomial volume growth, face inherent limitations when embedding data with strong hierarchical or tree-like structures [39]. The distances between nodes in deep branches of a hierarchy cannot be faithfully preserved without significant distortion. Hyperbolic geometry, defined by constant negative curvature and exponential volume growth, offers a mathematically principled solution, allowing hierarchical data to be embedded with significantly lower distortion [39, 31, 15]. This property has led to successful applications in various domains, including graph representation learning, natural language processing, and computer vision, as exemplified by recent work like HypCD for category discovery [42].

Despite these advances, many real-world systems present complexities beyond single-level hierarchies. Biological systems (molecule-cell-tissue), social networks (individual-group-organization), file systems, and complex software architectures all exhibit hierarchies *within* hierarchies – a fundamentally multi-scale structure. Furthermore, components within these hierarchies might possess intrinsic orientations (e.g., the 3D conformation of a protein subunit within a larger complex), and the relationships or transformations between different scales might involve rotations or specific geometric alignments. Current hyperbolic models, typically operating on a single manifold, struggle to capture this multi-scale nature and lack native mechanisms for efficient rotational modeling. Conversely, while Quaternions provide a compact and efficient algebra for rotations [43], and Quaternion Neural Networks (QNNs) leverage this for tasks involving orientation [44, 45], they operate within Euclidean (tangent) spaces and do not inherently possess the capacity to represent hierarchical structures efficiently.

This paper introduces **WuBu Nesting**, a conceptual framework designed to unify multi-scale hierarchical representation with potentially rotational inter-scale modeling. Departing from single-manifold or parallel product-space approaches ($\mathbb{H}^{n_1} \times \mathbb{H}^{n_2}$), WuBu Nesting proposes a *nested* structure where hyperbolic manifolds recursively contain other hyperbolic manifolds ($\mathbb{H}^{n_1} \supset \mathbb{H}^{n_2} \supset \dots$). This creates a deep, potentially fractal-like geometry. The core innovations enabling this structure are:

* **Adaptive Nesting:** The depth of nesting (number of levels) and the relative geometric scale ($s_i$) of each hyperbolic "bubble" are not fixed but are conceived as learnable parameters. This allows the model's geometry to adapt dynamically to the specific complexity and scale distribution present in the data.
* **Boundary Sub-Manifolds:** Each hierarchical level (hyperbolic ball) can host its own set of specialized, lower-dimensional manifolds (e.g., $\mathbb{H}^2$ disks or "circles"). These act as dedicated containers for representing distinct feature clusters, substructures, or modalities relevant at that particular scale.
* **Flexible Inter-Level Transformations:** The crucial links between nested levels are modeled by learnable functions operating between associated tangent spaces. These transformations are designed to be flexible, potentially incorporating quaternion networks (to explicitly model rotations and scaling between scales), simpler affine or non-linear projections, probabilistic Gaussian mappings (to handle uncertainty in transitions), or even dynamically learned "bridge manifolds" that create specific geometric pathways between regions across different scales.

The central hypothesis is that this adaptive, multi-scale, geometrically heterogeneous structure provides a significantly richer inductive bias. By explicitly modeling hierarchies at multiple resolutions and allowing for complex (potentially rotational) transformations between these scales, WuBu Nesting aims to learn more faithful and powerful representations for complex, structured data compared to existing approaches.

**2. Related Work**

**2.1 Hyperbolic Deep Learning**

The field was significantly advanced by Nickel and Kiela [39] with Poincaré embeddings, demonstrating the power of hyperbolic space for hierarchical graph data. Subsequent work extended this to various data types and tasks [31, 15, 1, 10]. A key focus has been adapting neural network components, such as linear layers [19] and attention mechanisms [22], to operate correctly within hyperbolic geometry (often via tangent space approximations). The HypCD work [42] exemplifies applying these techniques to capture implicit hierarchies in visual category discovery. However, a common limitation is the reliance on a single hyperbolic manifold, which may not adequately capture data with multiple distinct hierarchical scales or nested structures. WuBu Nesting directly addresses this by proposing a recursive nesting of manifolds.

**2.2 Quaternion Neural Networks (QNNs)**

Leveraging the quaternion algebra $\mathbb{Q}$ for representing rotations without gimbal lock [43], QNNs employ quaternion-valued weights, activations, and operations [44, 45]. They have proven effective in domains where orientation and rotation are critical, such as 3D computer vision, robotics simulation, and attitude control, often achieving better performance with fewer parameters compared to equivalent real-valued networks. Their primary limitation, in the context of WuBu Nesting, is their native operation within Euclidean spaces (or tangent spaces), lacking intrinsic mechanisms for modeling the large-scale hierarchical relationships that hyperbolic geometry captures so well. NQHG aims to integrate QNNs specifically for modeling the *transformations between* hierarchical levels represented in hyperbolic spaces.

**2.3 Product Manifolds and Multi-Scale Approaches**

Some research explores learning on product manifolds, such as $\mathbb{R}^n \times \mathbb{S}^m \times \mathbb{H}^k$, to combine the properties of different geometries [46 - *needs verification*]. While this allows simultaneous representation of different data aspects (e.g., Euclidean features, spherical orientations, hyperbolic hierarchies), the components exist in parallel. This differs fundamentally from WuBu Nesting's *nested* structure, where manifolds are contained within others, creating a depth dimension and enabling multi-scale analysis. Traditional multi-scale methods like image pyramids or wavelet transforms operate primarily in Euclidean space and lack the specific geometric properties (negative curvature, rotational algebra) that NQHG seeks to combine.

**3. The Enhanced WuBu Nesting Framework**

WuBu Nesting represents data within an adaptive, recursively nested structure of hyperbolic spaces, augmented with boundary sub-manifolds and connected by learnable, potentially quaternion-based, inter-level transformations.

**3.1. Conceptual Architecture**

The data processing flow involves mapping input features through progressively nested hyperbolic levels. Each level refines the representation at its specific scale and potentially interacts with specialized boundary manifolds. Information is then passed to deeper levels via learned transformation functions operating between tangent spaces. Finally, information from relevant scales is aggregated for downstream tasks.

```mermaid
graph TD;
    A[Input Data] --> B(Initial Encoding);
    B --> C{Map to Outer Tangent Space T_o(B1)};
    C --> D{Process/Transform in T_o(B1)};
    D --> E1[Map to Outer Ball B1 (exp_o^c, scale s1)];

    subgraph "Outer Ball B1 (Hyperbolic, Scale s1)"
        direction TB
        E1 --> F1{Intra-Ball Processing};
        F1 --> FB1{Boundary Manifolds (Circles) in B1};
        F1 --> G1{Identify Nesting Region / Map to Boundary Tangent Space T_p(B1)};
    end

    subgraph "Inter-Level Transformation T(1->2)"
        direction TB
        G1 & FB1 <--> H1(Learnable Transform Function: Quat, Proj, Gauss, Bridge...);
    end

    H1 --> I1{Map to Inner Tangent Space T_o(B2)};

    subgraph "Inner Ball B2 (Hyperbolic, Scale s2)"
        direction TB
        I1 --> J1[Map to Inner Ball B2 (exp_o^c, scale s2)];
        J1 --> K1{Intra-Ball Processing};
        K1 --> FB2{Boundary Manifolds (Circles) in B2};
        K1 --> L1{Identify Nesting Region / Map to Boundary Tangent Space T_p(B2)};
    end

     subgraph "Inter-Level Transformation T(2->3)"
        direction TB
        L1 & FB2 <--> H2(Learnable Transform Function ...);
    end

    H2 --> M1{... Adaptive Recursion ...};

    M1 --> N{Aggregate Information Across Levels/Scales};
    N --> O[Final Projection / Task Head];
    O --> P[Output];

    style E1 fill:#D6C0FF,stroke:#9966FF,stroke-width:2px
    style F1 fill:#D6C0FF,stroke:#9966FF,stroke-width:1px
    style FB1 fill:#E7DAFF,stroke:#9966FF,stroke-width:1px,stroke-dasharray: 5 5
    style J1 fill:#F5F0FF,stroke:#D6C0FF,stroke-width:2px
    style K1 fill:#F5F0FF,stroke:#D6C0FF,stroke-width:1px
    style FB2 fill:#E6E6FF,stroke:#D0D0FF,stroke-width:1px,stroke-dasharray: 5 5
    style H1 fill:#E7DAFF,stroke:#9966FF,stroke-width:2px,stroke-dasharray: 2 2
    style H2 fill:#E7DAFF,stroke:#9966FF,stroke-width:2px,stroke-dasharray: 2 2
Figure 1: Enhanced WuBu Nesting Conceptual Architecture with Adaptive Scales, Boundary Manifolds, and Flexible Transformations.3.2. Enhanced Component Details & Concepts3.2.1 Adaptive Nesting & ScalesThe core idea is that the geometry itself adapts. The number of nested levels, k, might be determined dynamically (e.g., via a learned stopping criterion) or set as a hyperparameter. The relative scale si​>0 of each ball Hci​ni​​ is a crucial learnable parameter. It controls the effective "zoom level" or resolution of that hierarchical layer. Learning si​ could involve direct gradient descent (if the loss function depends smoothly on si​) or reinforcement learning approaches. A scale-aware exponential map, such as:expo,si​ci​​(v)=tanh(si​⋅2ci​​∣v∣​)ci​​∣v∣v​directly incorporates si​ into the mapping from the tangent space (where Euclidean operations occur) to the manifold. Larger si​ values would map tangent vectors further into the ball, effectively increasing the hyperbolic distances between points near the origin, suitable for representing finer-grained hierarchies within that level. Conversely, smaller si​ create a relatively "flatter" local geometry near the origin. The interplay between scale si​ and curvature ci​ (which could also potentially be learnable per level) defines the specific geometric properties of each nested layer.3.2.2 Boundary Sub-ManifoldsInstead of just points, each level Hci​ni​​ can contain embedded, lower-dimensional manifolds Bi=Bi,1,…,Bi,mi​​. These are not just passive regions but active computational units.Role & Examples: They act as specialized "workspaces" or feature extractors for specific types of information relevant at scale i. In molecular modeling, one circle might represent aromatic rings, another protein backbones. In code analysis, different circles could correspond to loops, conditional statements, or specific API usage patterns.Parameterization: Their embedding function ϕi,j​:Hm→Hni​ (position, orientation), internal dimension m, and curvature c′ could be learned. This allows them to adapt their shape and location to best capture relevant substructures.Interaction: Information can be processed within these boundary manifolds using their own hyperbolic layers. They serve as key interfaces for the inter-level transformations, allowing structured information (not just point representations) about specific components to be passed between scales. For instance, the transformation Ti→i+1​ might operate differently depending on which boundary manifold Bi,j​ the information originates from.3.2.3 Boundary Detection/MappingThis module acts as the gatekeeper between levels. Given a representation pi′​ processed within Hci​ni​​, it needs to decide:Transition Trigger: Should we dive deeper? This could be based on the magnitude of pi′​ (points far from the origin might represent finer details needing a deeper level), the task requirements, or a learned gating function.Interface Selection: If transitioning, which boundary manifold Bi,j​ (if any) is the relevant interface? This could use an attention mechanism: Attn(p′i,Bi,j).Tangent Space Projection: Map the relevant information (either pi′​ itself or a representation derived from the selected Bi,j​) to the tangent space Tp​(Hni​) at the transition point p. This involves the logarithmic map and potentially parallel transport if the tangent space is not at the origin of the manifold definition.3.2.4 Flexible Inter-Level TransformationsThese functions Ti→i+1​:Tp​(Hni​)→To​(Hni+1​) are the core of cross-scale communication. Their flexibility is key:Quaternion Networks: v′=QuaternionTransform(v,si+1​). Explicitly models relative rotation and scaling between hierarchical levels. Essential if orientation matters across scales (e.g., aligning molecular subunits).MLP Projections: v′=MLP(v,si+1​). Provides general-purpose non-linear mapping capabilities. Useful when the relationship between scales is complex but not necessarily rotational.Gaussian Processes/Mappings: v′∼N(μ(v,si+1​),Σ(v,si+1​)). Introduces stochasticity, modeling uncertainty in cross-scale relationships or allowing for generative sampling across levels.Learned Bridge Manifolds: A more advanced concept where T itself learns an intermediate geometric pathway (potentially a simplified manifold or flow) connecting specific regions or boundary manifolds (e.g., Bi,j​ to Bi+1,l​). This could model specific, structured interactions between components at different scales.The choice could be fixed per model, selected via architecture search, or even dynamically chosen per transition using a learned gating mechanism based on the nature of the information being passed.3.2.5 Scale-Aware AggregationCombining information from representations p1′​,p2′​,…,pk′​ across different levels and scales requires careful handling of their different geometric contexts.Attention Across Levels: Map all relevant representations pi′​ back to a common reference tangent space (e.g., To​(Hn1​)) using sequences of inverse inter-level transformations and logarithmic maps. Then apply standard attention:Ki​,Vi​=MapToCommonTangent(pi′​)αi​=softmax(d​Qo​⋅Ki​​);output=i∑​αi​Vi​This allows the model to weigh the importance of information from different scales.Scale-Weighted Pooling: Apply pooling operations (e.g., mean, max) in the common tangent space, but weight the contribution of each level i by a function of its learned scale si​ or its relevance determined by another mechanism.Graph-Based Aggregation: View the nested levels and boundary manifolds as nodes in a dynamic graph. Use graph neural network techniques adapted for heterogeneous geometries to aggregate information across the structure.3.3. "Multi-Scale Nested Bubbles" AnalogyThe Russian doll analogy is enhanced: each doll (bubble) has a learnable size (scale si​) and can have intricate patterns (boundary manifolds Bi​) painted on it. The way one doll fits inside the next is not fixed but involves a flexible, learnable transformation (Ti→i+1​) that might rotate, stretch, or warp the connection.graph TD;
    subgraph "Outermost Bubble B1 (Scale s1)"
        style B1 fill:#D6C0FF,stroke:#9966FF,stroke-width:2px
        id1(Hierarchy/Features @ Scale 1)

        subgraph "Boundary Manifolds B1"
             style BM1 fill:#E7DAFF, stroke:#9966FF, stroke-dasharray: 5 5
             C1[Circle 1.1]
             C2[Circle 1.2]
        end

        subgraph "Inner Bubble B2 (Scale s2)"
             style B2 fill:#F5F0FF,stroke:#D6C0FF,stroke-width:2px
             id2(Hierarchy/Features @ Scale 2)
             subgraph "Boundary Manifolds B2"
                 style BM2 fill:#E6E6FF, stroke:#D0D0FF, stroke-dasharray: 5 5
                 C3[Circle 2.1]
             end
             id4(...)
        end
         T12["Transform T(1->2) (Scale s2, Quat/Proj/Gauss...)"]
         id1 -- T12 --> id2
         C1 -.-> T12
         C2 -.-> T12
         T12 -.-> C3

    end
Figure 2: Enhanced WuBu Nesting Multi-Scale Nested Space Concept with Boundary Circles and Learnable Scales/Transforms.Adaptive Scales: Bubbles dynamically resize.Specialized Substructures: Circles hold scale-specific features.Flexible Links: Transformations are diverse and learned.4. Mathematical FormulationFormalizing WuBu Nesting requires careful definitions:4.1 Nested Hyperbolic ManifoldA nested hyperbolic manifold Mnested​ is defined recursively. The base case is a single scaled Poincaré ball Hck​,sk​nk​​. For i<k:Mi=Hni​ci​,si​,Bi,NiWhere $\mathbb{H}^{n_i}{c_i, s_i}$ is the ball at level i, Bi=Bi,j is the set of boundary sub-manifolds within it, and Ni​ is the nesting function. Ni​ maps specific regions Ri⊂Hni​ci​,si​ (potentially associated with boundary manifolds $B{i,j}$) to the next inner manifold Mi+1​ via the transformation Ti→i+1​. The learnable parameters include ni​,ci​,si​, parameters of Bi​, and parameters of Ti→i+1​.4.2 Scale-Aware MetricsDefining a consistent distance metric dM​ on Mnested​ is complex.Intra-level distance dH,si​​(x,y) within a single ball Hci​,si​ni​​ must incorporate the scale si​. A potential form, derived from scaling the tangent space metric before mapping, is:dH,si​​(x,y)=ci​​1​cosh−1(1+2(1−ci​∣Logo,si​ci​(x)∣2/si2​)(1−ci​∣Logo,si​ci​(y)∣2/si2​)ci​∣∣Logo,si​ci​(x)−Logo,si​ci​(y)∣∣2​)(Note: This specific form requires rigorous derivation and verification.)Cross-level distance dcross​(xi​,yj​) between points xi​∈Hni​ and yj​∈Hnj​ (i<j) involves finding the shortest path through the intermediate transformations:dcross​(xi​,yj​)=pathmin​(dH,si​​(xi​,pi​)+l=i∑j−1​dT​(pl​,pl+1​)+dH,sj​​(pj​,yj​))where the path involves points pl​ at the boundaries/nesting regions, and dT​ represents the "length" or cost associated with traversing the transformation Tl→l+1​. Defining dT​ differentiably is a major challenge.4.3 Boundary ManifoldsEach Bi,j​∈Bi​ is defined by an embedding ϕi,j​:Hmc′→Hni​ci​,si​. This embedding must be learned, potentially constrained to be (approximately) isometric or conformal to preserve geometric properties. Operations can occur within the intrinsic geometry of Bi,j​ before information is passed via Ti→i+1​.5. Potential Applications and ExperimentsThe enhanced WuBu Nesting framework is particularly suited for:5.1 Multi-Resolution AnalysisDirectly modeling data at different, interacting levels of detail. Example: Analyzing satellite imagery where outer levels capture land cover types, inner levels capture building structures, and boundary manifolds capture road networks connecting them, with transformations modeling perspective shifts.5.2 Cross-Scale Transfer LearningLeveraging knowledge across scales. Example: Training a protein function predictor where one level models amino acid interactions (using hyperbolic distances), another models secondary structure orientations (using quaternion transformations between levels), and boundary manifolds represent active sites. Knowledge learned about folding patterns could transfer across levels.5.3 Generative ModelingGenerating realistic, complex structures. Example: Designing novel materials by sampling hierarchically within the nested geometry, where different levels control crystal structure, grain boundaries (boundary manifolds), and defect orientations (quaternion transformations).5.4 Interpretable AIProviding insights into model reasoning. Example: In analyzing source code, different nesting levels could automatically correspond to function, class, and module scopes, with boundary manifolds representing specific algorithms or data structures. The inter-level transformations would show how these components interact.6. Implementation ChallengesRealizing WuBu Nesting involves overcoming substantial hurdles:6.1 Mathematical FormalismRigorous definitions for the nested manifold structure, scale-aware metrics, cross-level distances, parallel transport across boundaries, and differentiable boundary manifold embeddings are needed. This likely requires advanced concepts from differential geometry.6.2 Numerical StabilityHyperbolic operations are sensitive, especially with multiple nested mappings and transformations. Techniques like careful initialization, gradient clipping (potentially Riemannian), stable implementations of exp/log maps, and possibly custom numerical precision formats are essential. Preventing scale parameters si​ from collapsing or exploding requires regularization.6.3 Computational CostThe recursive nature and potentially complex transformations significantly increase FLOPs compared to standard models. Efficient implementations, perhaps leveraging approximations or specialized hardware (if available for quaternion/hyperbolic ops), are needed. Parallelizing computations across levels and boundary manifolds is key.6.4 Memory UsageStoring activations and parameters for multiple nested levels, boundary manifolds, and complex transformation networks can lead to high memory demands. Gradient checkpointing, reversible layers, sparse representations of the geometry, or model parallelism strategies might be required.6.5 Optimization LandscapeOptimizing parameters across multiple, interacting geometric spaces with potentially adaptive structure presents a highly complex, non-convex landscape. Advanced optimizers (Riemannian Adam/SGD [4]), sophisticated initialization strategies, curriculum learning (starting with fewer levels/simpler transforms), and careful hyperparameter tuning are likely necessary.7. Proposed Implementation StrategyAn incremental approach is crucial:Fixed Two-Level Structure: Implement Hn1​⊃Hn2​ with fixed scales (s1​,s2​) and a simple inter-level transformation (e.g., MLP or basic Quaternion layer). Focus on stable implementation of mappings and tangent space operations. Validate on simple synthetic hierarchical data.Add Scale Learning: Introduce learnable s1​,s2​ with constraints (si​>0) and regularization. Test if scales adapt meaningfully.Add Boundary Manifolds: Implement a single, fixed H2 boundary manifold in Hn1​, and modify the transformation T1→2​ to optionally take input from it.Explore Transformation Options: Systematically implement and compare MLP, Quaternion, and potentially simplified Gaussian transformations for T1→2​. Evaluate performance and stability trade-offs.Full Adaptive Implementation: Tackle learnable nesting depth (e.g., using reinforcement learning or differentiable stopping criteria), multiple learnable boundary manifolds per level, and potentially adaptive selection of transformation types. This requires mature solutions to the stability and optimization challenges.8. Conclusion and Future WorkThe enhanced WuBu Nesting framework offers a conceptually powerful paradigm for adaptive, multi-scale geometric deep learning. By uniquely combining nested hyperbolic spaces, learnable scales, specialized boundary sub-manifolds, and flexible inter-level transformations (including quaternions), it holds the potential to model complex real-world systems with unprecedented fidelity, capturing intricate hierarchical and rotational dependencies simultaneously. While significant mathematical and engineering challenges remain, future work focused on rigorous formalization, stable and efficient implementation, and empirical validation promises to unlock new capabilities in representation learning for science, engineering, and artificial intelligence. WuBu Nesting represents a compelling, albeit ambitious, vision for the future of geometrically-informed machine learning.References[1] Atigh, M. G., Schoep, J., Acar, E., Van Noord, N., & Mettes, P. (2022). Hyperbolic image segmentation. CVPR.[4] Becigneul, G., & Ganea, O. E. (2019). Riemannian adaptive optimization methods. ICLR.[10] Chen, W., Han, X., Lin, Y., Zhao, H., Liu, Z., Li, P., Sun, M., & Zhou, J. (2022). Fully hyperbolic neural networks. ACL.[15] Ermolov, A., Mirvakhabova, L., Khrulkov, V., Sebe, N., & Oseledets, I. (2022). Hyperbolic vision transformers: Combining improvements in metric learning. CVPR.[19] Ganea, O., Bécigneul, G., & Hofmann, T. (2018). Hyperbolic neural networks. NeurIPS.[22] Gulcehre, C., Denil, M., Malinowski, M., Razavi, A., Pascanu, R., Hermann, K. M., ... & de Freitas, N. (2019). Hyperbolic attention networks. ICLR.[31] Khrulkov, V., Mirvakhabova, L., Ustinova, E., Oseledets, I., & Lempitsky, V. (2020). Hyperbolic image embeddings. CVPR.[39] Nickel, M., & Kiela, D. (2017). Poincaré embeddings for learning hierarchical representations. NeurIPS.[42] Liu, Y., He, Z., & Han, K. (2025). Hyperbolic Category Discovery. arXiv:2504.06120.[43] Hamilton, W. R. (1866). Elements of quaternions. Longmans, Green, & Company.[44] Parcollet, T., Morchid, M., Bousquet, P. M., Dufour, R., Linarès, G., & De Mori, R. (2019). Quaternion recurrent neural networks. ICLR.[45] Grassucci, E., Comminiello, D., & Uncini, A. (2021). Quaternion neural networks: State-of-the-art and research challenges. IEEE Transactions on Neural Networks and Learning Systems.[46] Gu,